apiVersion: v1
data:
  01-input.conf: "input {\n  http {\n    port => 8084\n    add_field => { \"cluster_name\"
    => \"${CLUSTER_NAME}\" }\n    threads => 5\n    id => \"customer_in\"\n  }\n}\n###
    Remove unneeded fields came from fluent-bit\nfilter {\n\tmutate {\n\t    remove_field
    => [\"date\", \"headers\", \"time\"]\n\t}\n}"
  02-filters.conf: filter {}
  99-outputs.conf: output { }
kind: ConfigMap
metadata:
  name: logstash-pipeline-customer-97tc4thm7t
  namespace: elastic-stack-logging
---
apiVersion: v1
data:
  cloudwatch.conf: "input {\n  http {\n    port => 8082\n    id => \"cw_in\"\n  }\n}\nfilter
    {\n\t### Remove unneeded fields came from fluent-bit\n  mutate {\n          remove_field
    => [ \"headers\",\"host\",\"date\"]\n      }\n\n  if [kubernetes] {\n  ### Add
    temp field to specify correct stream name\n    mutate {\n        add_field =>
    { \"[@metadata][stream_name]\" => \"%{[kubernetes][pod_name]}_%{[kubernetes][namespace_name]}_%{[kubernetes][container_name]}\"
    }\n      }\n  }\n### Add temp field to have the correct host value in log\n  if
    [private_ip] {\n    mutate {\n        add_field => { \"[@metadata][transf_host]\"
    => \"%{private_ip}\" }\n      }\n    mutate {\n        gsub => [ \"[@metadata][transf_host]\",
    \"\\.\", \"-\" ]\n        add_field => { \"host\" => \"ip-%{[@metadata][transf_host]}\"
    }\n      }\n  }      \n}\noutput {\n  if [kubernetes] {\n    awslogs {\n        region
    => \"${AWS_REGION}\"\n        log_group_name => \"/aws/containerinsights/${CW_CLUSTER_NAME}/application\"\n
    \       log_stream_name => \"%{[@metadata][stream_name]}\"\n        id => \"cw_app_out\"\n
    \     }\n  }\n  if [log_name] {\n    awslogs {\n        region => \"${AWS_REGION}\"\n
    \       log_group_name => \"/aws/containerinsights/${CW_CLUSTER_NAME}/host\"\n
    \       log_stream_name => \"%{log_name}-%{host}\"\n        id => \"cw_host_out\"\n
    \     }\n  }\n  if [systemd_unit] {\n    awslogs {\n        region => \"${AWS_REGION}\"\n
    \       log_group_name => \"/aws/containerinsights/${CW_CLUSTER_NAME}/dataplane\"\n
    \       log_stream_name => \"%{systemd_unit}-%{hostname}\"\n        id => \"cw_data_out\"\n
    \     }\n  }    \n}"
kind: ConfigMap
metadata:
  name: logstash-pipeline-cw-8chtm622kh
  namespace: elastic-stack-logging
---
apiVersion: v1
data:
  01-input.conf: "input {\n\tdead_letter_queue {\n\t    path => \"/usr/share/logstash/data/queue/dead_letter\"\n\t
    \   clean_consumed => true\n\t    commit_offsets => true\n\t    add_field => {\"DLQ\"
    => \"true\"}\n\t}\n}"
  02-filters.conf: filter {}
  99-outputs.conf: "output {\n\topensearch {\n\t\tid => \"OS_logstash_index\"\n\t\thosts
    => [\"https://opensearch-cluster-master.elastic-stack-logging.svc.cluster.local:9200\"]\n\t\tuser
    => \"${OS_USER}\"\n\t\tpassword => \"${OS_PASSWORD}\"\n\t\tssl => true\n\t\tssl_certificate_verification
    => false\n\t\tsniffing => false\n\t\tindex => \"logstash-%{+YYYY.MM.dd}\"\n\t}\n}"
kind: ConfigMap
metadata:
  name: logstash-pipeline-dlq-t7c6c88tfg
  namespace: elastic-stack-logging
---
apiVersion: v1
data:
  01-input.conf: |
    input {
      http {
        port => 8080
        add_field => { "cluster_name" => "${CLUSTER_NAME}" }
        id => 'main_in'
        threads => 10
      }
    }
  02-input-filters.conf: "filter {\n  ### Filter to not store logstash healthchecks\n
    \ ### Logstash will still return '200 OK' when readinessProbe tries to request
    the port,\n  ### but this request will not be stored anywhere\n  if ([headers][request_method]
    == \"GET\")\n  {\n    drop {}\n  }\n  ### Remove unneded fields coming from fluent-bit\n
    \ mutate {\n    remove_field => [\"date\", \"headers\", \"time\", \"_p\"]\n  }\n
    \ ### Fix ES\\OS issue with parsing dots in field names as nested objects\n  de_dot
    {\n    fields => [\"[kubernetes][labels][app.kubernetes.io]\",\n\t\t\t    \"[kubernetes][labels][app.kubernetes.io/name]\",\n\t\t\t
    \   \"[kubernetes][labels][app.kubernetes.io/version]\",\n\t\t\t    \"[kubernetes][labels][app.kubernetes.io/part-of]\",\n\t\t\t
    \   \"[kubernetes][labels][app.kubernetes.io/role]\",\n\t\t\t    \"[kubernetes][labels][app.kubernetes.io/component]\",\n\t\t\t
    \   \"[kubernetes][labels][app.kubernetes.io/instance]\",\n\t\t\t    \"[kubernetes][labels][statefulset.kubernetes.io/pod-name]\"\n
    \           ]\n  }\n  ### Backup jobs logs goes to separate index without parsing\n
    \ if ([kubernetes][pod_name] =~ \".*backup.*\" or [kubernetes][pod_name] =~ \".*repo1.*\")
    {\n      mutate {\n        rename => {\"log\" => \"message\"}\n        add_field
    => { \"[@metadata][index]\" => \"backup-jobs\"}\n      }\n  }\n  ### Healthchecks
    logs goes to separate index without parsing\n  else if ([kubernetes][pod_name]
    =~ \".*healthcheck.*\") {\n      mutate {\n        add_field => { \"[@metadata][index]\"
    => \"healthchecks\"}\n        rename => {\"log\" => \"message\"}\n      }\n  }\n}\n"
  03-ingress-filters.conf: |-
    filter {
      if ([kubernetes][container_name] == "nginx-ingress-controller"){
      ### Parsing for Nginx access logs
        if ([stream] == "stdout") {
          grok {
            match => {
              "log" => [ "%{IPORHOST:[nginx][access][remote_ip]} - %{USER:[nginx][access][user_name]} \[%{HTTPDATE:[nginx][access][time]}\] \"%{WORD:[nginx][access][method]} %{URIPATH:[nginx][access][url]} HTTP/%{NUMBER:[nginx][access][http_version]}\" %{NUMBER:[nginx][access][response_code]} %{NUMBER:[nginx][access][body_sent][bytes]} \"(?:-|%{URI:[nginx][access][referrer]})\" \"%{DATA:[nginx][access][agent]}\" %{NUMBER:[nginx][access][request_length]} %{NUMBER:[nginx][access][request_time]} \[%{NOTSPACE:[nginx][access][proxy_upstream_name]}\] \[\] (?:-|%{HOSTPORT:[nginx][access][upstream_addr]}) %{NOTSPACE:[nginx][access][upstream_response_length]} %{NOTSPACE:[nginx][access][upstream_response_time]} %{NOTSPACE:[nginx][access][upstream_response_code]} %{NOTSPACE:[nginx][access][req_id]}" ]
            }
            remove_field => ["log"]
            add_field => {"[@metadata][index]" => "ingress-access"}
          }
          date {
            match => [ "[nginx][access][time]", "dd/MMM/YYYY:H:m:s Z" ]
            remove_field => "[nginx][access][time]"
          }
          useragent {
            source => "[nginx][access][agent]"
            target => "[nginx][access][user_agent]"
            remove_field => "[nginx][access][agent]"
          }
          geoip {
            source => "[nginx][access][remote_ip]"
            target => "[nginx][access][geoip]"
          }
          mutate {
            gsub => ["[nginx][access][upstream_response_length]", "-", "0"]
          }
        }
        else if ([stream] == "stderr") {
        ### Parsing for Nginx error log
          if ([log] =~ /^[\d\/: ]+\[.+?\]/) {
            grok {
              match => { "log" => ["%{DATA:[nginx][error][time]} \[%{WORD:[nginx][error][level]}\] %{NUMBER:[nginx][error][pid]}#%{NUMBER:[nginx][error][tid]}: (\*%{NUMBER:[nginx][error][connection_id]} )?%{GREEDYDATA:[nginx][error][message]}"] }
              remove_field => ["log"]
              add_field => {"[@metadata][index]" => "ingress-error"}
            }
            date {
              match => [ "[nginx][error][time]", "YYYY/MM/dd H:m:s" ]
              remove_field => "[nginx][error][time]"
            }
          }
          ### Parsing for go Nginx-controller error log
          else if ([log] =~ /^\w{5} [\d:\.]{15}.[ \t]+.*?\]/) {
            grok {
              match => { "log" => ["[A-Z]%{DATA:timestamp} [ \t]+%{DATA}\:%{NUMBER}\] %{GREEDYDATA:[nginx][error][message]}"]}
              remove_field => ["log"]
              add_field => {"[@metadata][index]" => "ingress-error"}
            }
            date {
              match => ["timestamp", "MMdd HH:mm:ss.SSSSSS"]
              remove_field => ["timestamp"]
            }
          }
        }
      }
    }
  04-pa-filters.conf: |-
    filter {
        if ([kubernetes][container_name] =~ "pingaccess(-was)?(-admin)?") {
            ### Parse only logs starting from log path.
            ### Everything else(hooks, logs going directly to console, etc) goes to logstash index without any parsing
            if ([log] =~ /^\/opt\/out\/instance(\/\w+)?\/log[s]?/) {
                grok {
                    match => {
                        "log" => [ "^\/opt\/out\/instance(\/\w+)?\/log[s]?\/%{DATA:log_name}(\.log)?(\.out)? %{GREEDYDATA:log_string}" ]
                    }
                    remove_field => ["log"]
                }
                if ([log_string] == "") {
                    drop {}
                }
                if ([kubernetes][container_name] =~ "pingaccess-was(-admin)?"){
                    mutate {
                        add_field => {"[@metadata][app]" => "pa-was"}
                    }
                }
                else {
                    mutate {
                        add_field => {"[@metadata][app]" => "pa"}
                    }
                }
                if ([log_name] == "pingaccess_engine_audit") {
                  dissect {
                    mapping => {
                      "log_string" => "%{timestamp}| %{exchangeId}| %{trackingId}| %{roundTripMS} ms| %{proxyRoundTripMS} ms| %{resource}| %{subject}| %{authMech}| %{client}| %{method}| %{requestUri}| %{responseCode}| %{failedRuleType}| %{failedRuleName}| %{applicationName}| %{resourceName}| %{pathPrefix}"
                    }
                    remove_field => ["log_string"]
                    add_field => {"[@metadata][index]" => "%{[@metadata][app]}-engine-audit"}
                  }
                } else if ([log_name] == "pingaccess_api_audit") {
                  dissect {
                    mapping => {
                      "log_string" => "%{timestamp}| %{exchangeId}| %{trackingId}| %{roundTripMS} ms| %{subject}| %{authMech}| %{client}| %{method}| %{requestUri}| %{responseCode->}"
                    }
                    remove_field => ["log_string"]
                    add_field => {"[@metadata][index]" => "%{[@metadata][app]}-api-audit"}
                  }
                  mutate {
                    strip => ["responseCode"]
                  }
                }
                else if ([log_name] == "pingaccess_agent_audit") {
                  dissect {
                    mapping => {
                      "log_string" => "%{timestamp}| %{exchangeId}| %{trackingId}| %{roundTripMS} ms| %{resource}| %{client}| %{method}| %{requestUri}| %{responseCode}| %{applicationName}| %{resourceName}| %{pathPrefix}"
                    }
                    remove_field => ["log_string"]
                    add_field => {"[@metadata][index]" => "%{[@metadata][app]}-agent-audit"}
                  }
                }
                else if ([log_name] == "pingaccess_sideband_client_audit") {
                  dissect {
                    mapping => {
                      "log_string" => "%{timestamp}| %{exchangeId}| %{trackingId}| %{roundTripMS} ms| %{client}| %{method}| %{requestUri}| %{responseCode}| %{sidebandName}| %{sidebandDecision}"
                    }
                    remove_field => ["log_string"]
                    add_field => {"[@metadata][index]" => "%{[@metadata][app]}-sideband-client-audit"}
                  }
                }
                else if ([log_name] == "pingaccess_sideband_audit") {
                  dissect {
                    mapping => {
                      "log_string" => "%{timestamp}| %{exchangeId}| %{trackingId}| %{roundTripMS} ms| %{resource}| %{client}| %{method}| %{requestUri}| %{responseCode}| %{applicationName}| %{resourceName}| %{pathPrefix}| %{sidebandName}"
                    }
                    remove_field => ["log_string"]
                    add_field => {"[@metadata][index]" => "%{[@metadata][app]}-sideband-audit"}
                  }
                }
                else if ([log_name] == "pingaccess") {
                  grok {
                    match => {
                      "log_string" => [ "%{TIMESTAMP_ISO8601:timestamp}[ ]{1,2}%{WORD:logLevel} \[(?:|%{NOTSPACE:exchangeId})\] %{NOTSPACE:className} - %{GREEDYDATA:message}" ]
                    }
                    remove_field => ["log_string"]
                    add_field => {"[@metadata][index]" => "%{[@metadata][app]}-pingaccess"}
                  }
                }
                else if ([log_name] == "audit") {
                  grok {
                    match => {
                      "log_string" => [ "%{TIMESTAMP_ISO8601:timestamp} %{GREEDYDATA:message}"]
                    }
                    remove_field => ["log_string"]
                    add_field => {"[@metadata][index]" => "%{[@metadata][app]}-upgrade-audit"}
                  }
                }
                else if ([log_name] == "upgrade") {
                  grok {
                    match => {
                      "log_string" => [ "%{TIMESTAMP_ISO8601:timestamp}[ ]{1,2}%{WORD:logLevel} %{NOTSPACE:className}:%{NUMBER:position} - %{GREEDYDATA:message}"]
                    }
                    remove_field => ["log_string"]
                    add_field => {"[@metadata][index]" => "%{[@metadata][app]}-upgrade-log"}
                  }
                }
                else if ([log_name] == "upgrade_status") {
                  mutate {
                    rename => { "log_string" => "message" }
                    add_field => {"[@metadata][index]" => "%{[@metadata][app]}-upgrade-status"}
                  }
                }
                ### For log files we don't know right now - store in separate indices without parsing
                else {
                  mutate {
                    rename => { "log_string" => "message" }
                    add_field => {"[@metadata][index]" => "%{[@metadata][app]}-%{[log_name]}"}
                  }
                }
                if ([timestamp]) {
                  date {
                    match => [ "timestamp", "ISO8601", "yyyy-MM-dd HH:mm:ss"]
                    remove_field => ["timestamp"]
                  }
                }
                ### Security Enrichments begin here, ENRICH THE IP ADDRESS DETAIL
                if ([client]) {
                    geoip
                    {
                        source => "client"
                    }
                    translate {
                        source => "client"
                        target => "threat_intel"
                        fallback => "No"
                        dictionary_path => '/enrichment-cache-files/AlienVaultIP.yml'
                        refresh_behaviour => "replace"
                    }
                    translate {
                        source => "client"
                        target => "tor_intel"
                        fallback => "No"
                        dictionary_path => '/enrichment-cache-files/TorNodes.yml'
                        refresh_behaviour => "replace"
                    }
                    translate {
                        source => "[geoip][country_name]"
                        target => "malicious_country"
                        fallback => "No"
                        dictionary_path => '/enrichment-cache-files/MaliciousCountries.yml'
                        refresh_behaviour => "replace"
                    }
                    translate {
                        source => "[geoip][country_name]"
                        target => "known_country"
                        fallback => "No"
                        dictionary_path => '/enrichment-cache-files/KnownCountries.yml'
                        refresh_behaviour => "replace"
                    }
                    if([malicious_country] == "No" and [known_country] == "No"){
                        mutate {
                          add_field => { "suspicious_country" => "YES" }
                        }
                    }
                }
            }
        }
    }
  05-pd-filters.conf: "filter {\n    if ([kubernetes][container_name] =~ \"pingdirectory\")
    {\n    ### Parse only logs starting from log path.\n    ### Everything else(hooks,
    logs going directly to console, etc) goes to logstash index without any parsing\n
    \       if ([log] =~ /^\\/opt\\/out\\/instance\\/logs/) {\n            grok {\n
    \               match => {\n                  \"log\" => [ \"^\\/opt\\/out\\/instance(\\/\\w+)?\\/log[s]?\\/%{DATA:log_name}(\\.log)?(\\.out)?
    %{GREEDYDATA:log_string}\" ]\n                }\n                remove_field
    => [\"log\"]\n            }\n            if ([log_string] == \"\") {\n                drop
    {}\n            }\n            mutate {\n                add_field => {\"[@metadata][index]\"
    => \"pd-%{log_name}\"}\n            }\n            if ([log_name] == \"access\"
    ) {\n                kv {\n                  source => \"[log_string]\"\n                  value_split
    => \"=\"\n                  include_brackets => true\n                }\n                grok
    {\n                  match => { \"log_string\" => \"\\[%{DATA:timestamp}\\] %{DATA:ldapType}
    %{WORD}=%{GREEDYDATA}\" }\n                  remove_field => [\"log_string\"]\n
    \               }\n                date {\n                  match => [\"timestamp\",
    \"dd/MMM/yyyy:HH:mm:ss.SSS Z\", \"dd/MMM/yyyy:HH:mm:ss Z\"]\n                  remove_field
    => [\"timestamp\"]\n                }\n                mutate{\n                  gsub
    => [ \"filter\", '\"', \"\" ]\n                  gsub => [ \"dn\", '\"', \"\"
    ]\n                  gsub => [ \"requesterIP\", \"internal\", \"127.0.0.1\" ]\n
    \                 rename => { \"msg\" => \"message\" }\n                }\n                ###
    Security Enrichments begin here, ENRICH THE IP ADDRESS DETAIL\n                translate
    {\n                  source => \"requesterIP\"\n                  target => \"threat_intel\"\n
    \                 fallback => \"No\"\n                  dictionary_path => '/enrichment-cache-files/AlienVaultIP.yml'\n
    \                 refresh_behaviour => \"replace\"\n                }\n                translate
    {\n                  source => \"requesterIP\"\n                  target => \"tor_intel\"\n
    \                 fallback => \"No\"\n                  dictionary_path => '/enrichment-cache-files/TorNodes.yml'\n
    \                 refresh_behaviour => \"replace\"\n                }\n                translate
    {\n                  source => \"[geoip][country_name]\"\n                  target
    => \"malicious_country\"\n                  fallback => \"No\"\n                  dictionary_path
    => '/enrichment-cache-files/MaliciousCountries.yml'\n                  refresh_behaviour
    => \"replace\"\n                }\n                translate {\n                  source
    => \"[geoip][country_name]\"\n                  target => \"known_country\"\n
    \                 fallback => \"No\"\n                  dictionary_path => '/enrichment-cache-files/KnownCountries.yml'\n
    \                 refresh_behaviour => \"replace\"\n                }\n                if([malicious_country]
    == \"No\" and [known_country] == \"No\"){\n                    mutate {\n                        add_field
    => { \"suspicious_country\" => \"YES\" }\n                    }\n                }\n
    \           }\n            else if ([log_name] == \"server\" or [log_name] ==
    \"errors\" or [log_name] == \"replication\") {\n                grok {\n                    match
    => {\n                        \"log_string\" => [ \"\\[%{DATA:timestamp}\\] %{WORD:severity}
    %{GREEDYDATA:message}\",\"\\[%{DATA:timestamp}\\][ ]{1,2}%{WORD}=%{GREEDYDATA}\"]\n
    \                   }\n                    break_on_match => \"true\"\n                }\n
    \               if ![severity] {\n                    kv {\n                        source
    => \"[log_string]\"\n                        value_split => \"=\"\n                        field_split
    => \" \\n\"\n                        include_brackets => true\n                    }\n
    \               }\n                date {\n                  match => [\"timestamp\",
    \"dd/MMM/yyyy:HH:mm:ss.SSS Z\", \"dd/MMM/yyyy:HH:mm:ss Z\"]\n                  remove_field
    => [\"timestamp\"]\n                }\n                if \"_grokparsefailure\"
    not in [tags] {\n\t                mutate{\n\t                  remove_field =>
    [\"log_string\"]\n\t                }\n                }\n            }\n            else
    if ([log_name] == \"failed-ops\" or [log_name] == \"expensive-write-ops\"){\n
    \               kv {\n                  source => \"[log_string]\"\n                  value_split
    => \"=\"\n                  include_brackets => true\n                }\n                grok
    {\n                  match => { \"log_string\" => \"\\[%{DATA:timestamp}\\] %{DATA:ldapType}
    %{WORD}=%{GREEDYDATA}\" }\n                  remove_field => [\"log_string\"]\n
    \               }\n                date {\n                  match => [\"timestamp\",
    \"dd/MMM/yyyy:HH:mm:ss.SSS Z\", \"dd/MMM/yyyy:HH:mm:ss Z\"]\n                  remove_field
    => [\"timestamp\"]\n                }\n            }\n        }\n    }\n}"
  06-pds-filters.conf: "filter {\n    if ([kubernetes][container_name] =~ \"pingdatasync\")
    {\n    ### Parse only logs starting from log path.\n    ### Everything else(hooks,
    logs going directly to console, etc) goes to logstash index without any parsing\n
    \       if ([log] =~ /^\\/opt\\/out\\/instance\\/logs/) {\n            grok {\n
    \               match => {\n                  \"log\" => [ \"^\\/opt\\/out\\/instance(\\/\\w+)?\\/log[s]?\\/%{DATA:log_name}(\\.log)?(\\.out)?
    %{GREEDYDATA:log_string}\" ]\n                }\n                remove_field
    => [\"log\"]\n            }\n            if ([log_string] == \"\") {\n                drop
    {}\n            }\n            mutate {\n                add_field => {\"[@metadata][index]\"
    => \"pds-%{log_name}\"}\n            }\n            if ([log_name] == \"access\"
    ) {\n                kv {\n                  source => \"[log_string]\"\n                  value_split
    => \"=\"\n                  include_brackets => true\n                }\n                grok
    {\n                  match => { \"log_string\" => \"\\[%{DATA:timestamp}\\] %{DATA:ldapType}
    %{WORD}=%{GREEDYDATA}\" }\n                  remove_field => [\"log_string\"]\n
    \               }\n                date {\n                  match => [\"timestamp\",
    \"dd/MMM/yyyy:HH:mm:ss.SSS Z\", \"dd/MMM/yyyy:HH:mm:ss Z\"]\n                  remove_field
    => [\"timestamp\"]\n                }\n                mutate{\n                  gsub
    => [ \"filter\", '\"', \"\" ]\n                  gsub => [ \"dn\", '\"', \"\"
    ]\n                  gsub => [ \"requesterIP\", \"internal\", \"127.0.0.1\" ]\n
    \                 rename => { \"msg\" => \"message\" }\n                }\n                translate
    {\n                  source => \"requesterIP\"\n                  target => \"threat_intel\"\n
    \                 fallback => \"No\"\n                  dictionary_path => '/enrichment-cache-files/AlienVaultIP.yml'\n
    \                 refresh_behaviour => \"replace\"\n                }\n                translate
    {\n                  source => \"requesterIP\"\n                  target => \"tor_intel\"\n
    \                 fallback => \"No\"\n                  dictionary_path => '/enrichment-cache-files/TorNodes.yml'\n
    \                 refresh_behaviour => \"replace\"\n                }\n                translate
    {\n                  source => \"[geoip][country_name]\"\n                  target
    => \"malicious_country\"\n                  fallback => \"No\"\n                  dictionary_path
    => '/enrichment-cache-files/MaliciousCountries.yml'\n                  refresh_behaviour
    => \"replace\"\n                }\n                translate {\n                  source
    => \"[geoip][country_name]\"\n                  target => \"known_country\"\n
    \                 fallback => \"No\"\n                  dictionary_path => '/enrichment-cache-files/KnownCountries.yml'\n
    \                 refresh_behaviour => \"replace\"\n                }\n                if([malicious_country]
    == \"No\" and [known_country] == \"No\"){\n                    mutate {\n                        add_field
    => { \"suspicious_country\" => \"YES\" }\n                    }\n                }\n
    \           }\n            else if ([log_name] == \"server\" or [log_name] ==
    \"errors\") {\n                grok {\n                    match => {\n                        \"log_string\"
    => [ \"\\[%{DATA:timestamp}\\] %{WORD:severity} %{GREEDYDATA:message}\",\"\\[%{DATA:timestamp}\\]
    %{WORD}=%{GREEDYDATA}\"]\n                    }\n                    break_on_match
    => \"true\"\n                }\n                if ![severity] {\n\t                kv
    {\n\t                    source => \"[log_string]\"\n\t                    value_split
    => \"=\"\n\t                    field_split => \" \\n\"\n\t                    include_brackets
    => true\n\t                }\n\t            }\n                date {\n                  match
    => [\"timestamp\", \"dd/MMM/yyyy:HH:mm:ss.SSS Z\", \"dd/MMM/yyyy:HH:mm:ss Z\"]\n
    \                 remove_field => [\"timestamp\"]\n                }\n                if
    \"_grokparsefailure\" not in [tags] {\n\t                mutate{\n\t                  remove_field
    => [\"log_string\"]\n\t                }\n                }\n            }\n            else
    if ([log_name] == \"failed-ops\"){\n                kv {\n                  source
    => \"[log_string]\"\n                  value_split => \"=\"\n                  include_brackets
    => true\n                }\n                grok {\n                  match =>
    { \"log_string\" => \"\\[%{DATA:timestamp}\\] %{DATA:ldapType} %{WORD}=%{GREEDYDATA}\"
    }\n                  remove_field => [\"log_string\"]\n                }\n                date
    {\n                  match => [\"timestamp\", \"dd/MMM/yyyy:HH:mm:ss.SSS Z\",
    \"dd/MMM/yyyy:HH:mm:ss Z\"]\n                  remove_field => [\"timestamp\"]\n
    \               }\n            }\n            else if ([log_name] == \"sync-failed-ops\"){\n
    \               grok {\n                    match => {\"log_string\" => \"\\[%{DATA:timestamp}\\]
    %{GREEDYDATA:message}\"}\n                    remove_field => [\"log_string\"]\n
    \               }\n                date {\n                  match => [\"timestamp\",
    \"dd/MMM/yyyy:HH:mm:ss.SSS Z\", \"dd/MMM/yyyy:HH:mm:ss Z\"]\n                  remove_field
    => [\"timestamp\"]\n                }\n            }\n        }\n    }\n}"
  07-pf-filters.conf: "filter {\n    if ([kubernetes][container_name] =~ \"pingfederate(-admin)?\")
    {\n    ### Parse only logs starting from log path.\n    ### Everything else(hooks,
    logs going directly to console, etc) goes to logstash index without any parsing\n
    \       if ([log] =~ /^\\/opt\\/out\\/instance\\/log/) {\n            grok {\n
    \               match => {\n                    \"log\" => [ \"^\\/opt\\/out\\/instance\\/log\\/%{DATA:log_name}(\\.log)?(\\.out)?
    %{GREEDYDATA:log_string}\" ]\n                }\n                remove_field
    => [\"log\"]\n            }\n            if ([log_string] == \"\") {\n                drop
    {}\n            }\n            mutate {\n                add_field => {\"[@metadata][index]\"
    => \"pf-%{log_name}\"}\n            }\n            if ([log_name] == \"admin-api\"
    or [log_name] == \"runtime-api\") {\n              dissect {\n                mapping
    => {\n                  \"log_string\" => \"%{timestamp}| %{user}| %{authType}|
    %{clientIP}| %{httpMethod}| %{url}| %{status}\"\n                }\n                remove_field
    => [\"log_string\"]\n              }\n              mutate {\n                gsub
    => [\"clientIP\",\"[\\[\\]]\",\"\"]\n              }\n            }\n            else
    if ([log_name] == \"admin-event-detail\") {\n              dissect {\n                mapping
    => {\n                  \"log_string\" => \"%{eventDetailId} | %{filename} | %{delta}
    | %{position} | %{message}\"\n                }\n                remove_field
    => [\"log_string\"]\n              }\n            }\n            else if ([log_name]
    == \"admin\") {\n              dissect {\n                mapping => {\n                  \"log_string\"
    => \"%{timestamp} | %{user} | %{roles} | %{ip} | %{component} | %{event} | %{eventDetailId}
    | %{message}\"\n                }\n                remove_field => [\"log_string\"]\n
    \             }\n              mutate {\n                replace => {\"[@metadata][index]\"
    => \"pf-admin-log\"}\n                gsub => [\"ip\",\"[\\[\\]]\",\"\"]\n              }\n
    \           }\n            else if ([log_name] == \"transaction\") {\n              dissect
    {\n                mapping => {\n                  \"log_string\" => \"%{timestamp}
    \ | %{host} | %{log_level} | %{event} | Connection ID: %{connectionId} | Virtual
    Server ID: %{virtualServerId} | %{urn} | %{eventType} | SAML ID: %{samlId} | %{variable_part}\"\n
    \               }\n                remove_field => [\"log_string\"]\n              }\n
    \             grok {\n                match => {\n                  \"variable_part\"
    => \"(Endpoint: %{URI:endpoint})|(SAML Subject: %{DATA:samlSubject} \\| %{DATA:binding}
    \\| SignatureStatus: %{DATA:signatureStatus}( \\|%{DATA:xmlMessage})?)$\"\n                }\n
    \               remove_field => [\"variable_part\"]\n              }\n            }\n
    \           else if ([log_name] == \"audit\") {\n              dissect {\n                mapping
    => {\n                  \"log_string\" => \"%{timestamp}| %{trackingId}| %{event}|
    %{subject}| %{ip} | %{app}| %{connectionId}| %{protocol}| %{host}| %{role}| %{status}|
    %{adapterId}| %{description}| %{responseTime->}\"\n                }\n                remove_field
    => [\"log_string\"]\n              }\n              ruby {\n                code
    => '\n                    hash = event.to_hash\n                    hash.each
    do |k,v|\n\t                    if v == \"\" || v == nil\n\t                        event.remove(k)\n\t
    \                   end\n                    end\n                '\n              }\n
    \           }\n            else if ([log_name] == \"provisioner-audit\") {\n              dissect
    {\n                mapping => {\n                  \"log_string\" => \"%{timestamp}|
    %{cycleId}| %{channelId}| %{eventType}| %{sourceId}| %{targetId}| %{isSuccess}|
    %{nonSuccessCause}\"\n                }\n                remove_field => [\"log_string\"]\n
    \             }\n            }\n            else if ([log_name] == \"provisioner\")
    {\n              grok {\n                match => {\n                  \"log_string\"
    => \"%{DATA}[ ]{1,3}%{WORD:logLevel}[ ]{1,3}\\[%{NOTSPACE:className}\\] %{GREEDYDATA:message}\"\n
    \               }\n                remove_field => [\"log_string\"]\n              }\n\t
    \         mutate {\n\t            replace => {\"[@metadata][index]\" => \"pf-provisioner-log\"}\n\t
    \           gsub => [\"ip\",\"[\\[\\]]\",\"\"]\n\t          }\n            }\n
    \           else if ([log_name] == \"server\") {\n              grok {\n                match
    => {\n                  \"log_string\" => \"%{TIMESTAMP_ISO8601:timestamp}( %{DATA:trackingId})?
    %{WORD:logLevel}[ ]{1,3}\\[%{NOTSPACE:className}\\] %{GREEDYDATA:message}\"\n
    \               }\n                remove_field => [\"log_string\"]\n              }\n
    \           }\n            else if ([log_name] == \"init\") {\n              grok
    {\n                match => {\n                  \"log_string\" => \"%{TIMESTAMP_ISO8601:timestamp}
    %{GREEDYDATA:message}\"\n                }\n                remove_field => [\"log_string\"]\n
    \             }\n            }\n            else if ([log_name] == \"jvm-garbage-collection\")
    {\n              grok {\n                match => {\n                  \"log_string\"
    => \"\\[%{TIMESTAMP_ISO8601:timestamp}\\]\\[%{NUMBER:secondsFromStart}s\\]\\[%{WORD:logLevel}\\]\\[%{DATA:categories}\\]
    %{GREEDYDATA:message}\"\n                }\n                remove_field => [\"log_string\"]\n
    \             }\n            }\n            else if ([log_name] =~ \"request\")
    {\n              grok {\n                match => {\n                  \"log_string\"
    => \"(%{IPORHOST:clientip})? %{HTTPDUSER:ident} %{USER:auth} \\[%{HTTPDATE:timestamp}\\]
    \\\"(?:%{WORD:method} %{NOTSPACE:request}(?: HTTP/%{NUMBER:httpversion})?|%{DATA:rawrequest})\\\"
    %{NUMBER:response} (?:%{NUMBER:bytes}|- )( \\\"%{NOTSPACE:referer}\\\")?( \\\"%{NOTSPACE:useragent}\\\")?\"\n
    \               }\n                remove_field => [\"log_string\"]\n              }\n
    \             mutate {\n                replace => {\"[@metadata][index]\" =>
    \"pf-request\"}\n              }\n            }\n            if ([timestamp])
    {\n              date {\n                match => [ \"timestamp\", \"ISO8601\",
    \"yyyy-MM-dd HH:mm:ss\", \"HH:mm:ss,SSS\", \"dd/MMM/yyyy:HH:mm:ss Z\", \"yyyy-MM-dd
    HH:mm:ss,SSS\"]\n                remove_field => [\"timestamp\"]\n              }\n
    \           }\n            if ([ip]) {\n                geoip {\n                    source
    => \"ip\"\n                }\n              ### Security Enrichments begin here,
    ENRICH THE IP ADDRESS DETAIL\n                translate {\n                    source
    => \"ip\"\n                    target => \"threat_intel\"\n                    fallback
    => \"No\"\n                    dictionary_path => '/enrichment-cache-files/AlienVaultIP.yml'\n
    \                   refresh_behaviour => \"replace\"\n                }\n                translate
    {\n                    source => \"ip\"\n                    target => \"tor_intel\"\n
    \                   fallback => \"No\"\n                    dictionary_path =>
    '/enrichment-cache-files/TorNodes.yml'\n                    refresh_behaviour
    => \"replace\"\n                }\n                translate {\n                    source
    => \"[geoip][country_name]\"\n                    target => \"malicious_country\"\n
    \                   fallback => \"No\"\n                    dictionary_path =>
    '/enrichment-cache-files/MaliciousCountries.yml'\n                    refresh_behaviour
    => \"replace\"\n                }\n                translate {\n                    source
    => \"[geoip][country_name]\"\n                    target => \"known_country\"\n
    \                   fallback => \"No\"\n                    dictionary_path =>
    '/enrichment-cache-files/KnownCountries.yml'\n                    refresh_behaviour
    => \"replace\"\n                }\n                if([malicious_country] == \"No\"
    and [known_country] == \"No\"){\n                    mutate {\n                        add_field
    => { \"suspicious_country\" => \"YES\" }\n                    }\n                }\n
    \             #Query for previous logins in OpenSearch, if login is found append
    data to the log\n              #IF A SUCCESSFUL LOGIN OCCURS, Query OS to see
    if the the attempt was successful in the past to determine distance from previous
    login.\n                if([status] == \"success\" and [event] == \"AUTHN_ATTEMPT\"
    and \"_geoip_lookup_failure\" not in [tags]){\n                    opensearch
    {\n                        index => \"pf-audit-read\"\n                        query_template
    => \"/etc/logstash/templates/6hr-1200km-template.json\"\n                        hosts
    \      => [\"opensearch-cluster-master.elastic-stack-logging.svc.cluster.local:9200\"]\n
    \                       user => \"${OS_USER}\"\n                        password
    => \"${OS_PASSWORD}\"\n                        ssl         => true\n                        add_field
    => {\"found_distance_alert\" => \"YES\"}\n                        fields => {\n
    \                           \"subject\" => \"found_subject\"\n                            \"ip\"
    => \"found_ip\"\n                            \"[geoip][country_name]\" => \"found_country\"\n
    \                           \"[geoip][city_name]\" => \"found_city_name\"\n                            \"[geoip][latitude]\"
    => \"[found_geoip][latitude]\"\n                            \"[geoip][longitude]\"
    => \"[found_geoip][longitude]\"\n                            \"[geoip][location]\"
    => \"[found_geoip][location]\"\n                        }\n                    }\n
    \                   ### transform geoip coordinates to geoshape line between two
    locations\n                    if ([found_distance_alert] == \"YES\") {\n                        mutate
    {\n                            add_field => {\"[found_geoip][line]\" => \"LINESTRING
    (%{[geoip][longitude]} %{[geoip][latitude]},%{[found_geoip][longitude]} %{[found_geoip][latitude]})\"}\n
    \                       }\n                    }\n                }\n            }\n
    \       }\n    }\n}"
  08-pc-filters.conf: "filter {\n\tif ([kubernetes][container_name] == \"pingcentral\")
    {\n    ### Parse only logs starting from log path.\n    ### Everything else(hooks,
    logs going directly to console, etc) goes to logstash index without any parsing\n\t\tif
    ([log] =~ \"^\\/opt\\/out\\/instance(\\/\\w+)?\\/log[s]?\") {\n\t\t\tgrok {\n\t\t\t\tmatch
    => {\n\t\t\t\t\t\"log\" => [ \"^\\/opt\\/out\\/instance(\\/\\w+)?\\/log[s]?\\/%{DATA:log_name}(\\.log)?(\\.out)?
    %{GREEDYDATA:log_string}\" ]\n\t\t\t\t}\n\t\t\t\tremove_field => [\"log\"]\n\t\t\t}\n
    \           if ([log_string] == \"\") {\n                drop {}\n            }\n\t\t\tmutate
    {\n\t\t\t\tadd_field => {\"[@metadata][index]\" => \"pc-%{log_name}\"}\n\t\t\t}\n\t\t\tif
    ([log_name] == \"application-ext\" or [log_name] == \"application\") {\n\t\t\t\tgrok
    {\n\t\t\t\t\tmatch => {\n\t\t\t\t\t\t\"log_string\" => \"%{TIMESTAMP_ISO8601:timestamp}[
    ]{1,3}%{WORD:logLevel}[ ]{1,3}%{NOTSPACE:className}:%{NUMBER:position} - %{GREEDYDATA:message}\"\n\t\t\t\t\t}\n\t\t\t\t\tremove_field
    => [\"log_string\"]\n\t\t\t\t}\n\t\t\t}\n\t\t\telse if ([log_name] == \"monitor\")
    {\n\t\t\t\tgrok {\n\t\t\t\t\tmatch => {\n\t\t\t\t\t\t\"log_string\" => \"%{TIMESTAMP_ISO8601:timestamp}
    \\| %{NOTSPACE:className} %{GREEDYDATA:message}\"\n\t\t\t\t\t}\n\t\t\t\t\tremove_field
    => [\"log_string\"]\n\t\t\t\t}\n\t\t\t}\n\t\t\telse if ([log_name] == \"application-api\")
    {\n\t\t\t\tdissect {\n\t\t\t\t\tmapping => {\n\t\t\t\t\t\t\"log_string\" => \"%{timestamp}
    | %{className} %{user} | %{method} | %{client} | %{requestUri} | %{responseCode}\"\n\t\t\t\t\t}\n\t\t\t\t\tremove_field
    => [\"log_string\"]\n\t\t\t\t}\n\t\t\t}\n\t\t\tif ([timestamp]) {\n\t\t\t\tdate
    {\n\t\t\t\t\tmatch => [ \"timestamp\", \"ISO8601\", \"yyyy-MM-dd HH:mm:ss\" ]\n\t\t\t\t\tremove_field
    => [\"timestamp\"]\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}"
  09-pdg-filters.conf: "filter {\nif ([kubernetes][container_name] == \"pingdelegator\")
    {\n    ### Parse only logs starting from log path.\n    ### Everything else(hooks,
    logs going directly to console, etc) goes to logstash index without any parsing\n\t\tif
    ([log] =~ \"^\\/opt\\/out\\/instance(\\/\\w+)?\\/log[s]?\") {\n\t\t\tgrok {\n\t\t\t\tmatch
    => {\n\t\t\t\t\t\"log\" => [ \"^\\/opt\\/out\\/instance(\\/\\w+)?\\/log[s]?\\/%{DATA:log_name}(\\.log)?(\\.out)?
    %{GREEDYDATA:log_string}\" ]\n\t\t\t\t}\n\t\t\t\tremove_field => [\"log\"]\n\t\t\t}\n
    \           if ([log_string] == \"\") {\n                drop {}\n            }\n\t\t\tmutate
    {\n\t\t\t\tadd_field => {\"[@metadata][index]\" => \"pdg-%{log_name}\"}\n\t\t\t}\n\t\t\tif
    ([log_name] == \"access\") {\n\t\t\t\tdissect {\n\t\t\t\t\tmapping => {\n\t\t\t\t\t\t\"log_string\"
    => '%{client} - %{user} [%{timestamp}] \"%{method} %{url} HTTP/%{httpVersion}\"
    %{responseCode} %{bodySentBytes} \"%{referrer}\" \"%{userAgent}\" \"%{httpForwardedFor}\"'\n\t\t\t\t\t}\n\t\t\t\t\tremove_field
    => [\"log_string\"]\n\t\t\t\t}\n\t\t\t}\n\t\t\telse if ([log_name] == \"error\")
    {\n\t\t\t\tgrok {\n\t\t\t\t\tmatch => {\n\t\t\t\t\t\t\"log_string\" => [\"%{DATA:timestamp}
    \\[%{WORD:logLevel}\\] %{NUMBER:pid}#%{NUMBER:tid}: (\\*%{NUMBER:connectionId}
    )?%{GREEDYDATA:message}\"]\n\t\t\t\t\t}\n\t\t\t\t\tremove_field => [\"log_string\"]\n\t\t\t\t}\n\t\t\t}\n\t\t\tif
    ([timestamp]){\n\t\t\t\tdate {\n\t\t\t\t\tmatch => [ \"timestamp\", \"dd/MMM/yyyy:H:m:s
    Z\", \"yyyy/MM/dd HH:mm:ss\" ]\n\t\t\t\t\tremove_field => [\"timestamp\"]\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}"
  10-outputs.conf: |-
    output {
        if ([@metadata][index]) {
            opensearch {
              id => "OS_specified_index"
              hosts => ["https://opensearch-cluster-master.elastic-stack-logging.svc.cluster.local:9200"]
              user => "${OS_USER}"
              password => "${OS_PASSWORD}"
              ssl => true
              ssl_certificate_verification => false
              sniffing => false
              index => "%{[@metadata][index]}-%{+YYYY.MM.dd}"
            }
        }
        else {
            opensearch {
              id => "OS_everything"
              hosts => ["https://opensearch-cluster-master.elastic-stack-logging.svc.cluster.local:9200"]
              user => "${OS_USER}"
              password => "${OS_PASSWORD}"
              ssl => true
              ssl_certificate_verification => false
              sniffing => false
              index => "logstash-%{+YYYY.MM.dd}"
            }
        }
    }
kind: ConfigMap
metadata:
  name: logstash-pipeline-main-ht5h988bg2
  namespace: elastic-stack-logging
---
apiVersion: v1
data:
  newrelic.conf: "input {\n  http {\n    port => 8083\n    id => \"NR_in\"\n  }\n}\nfilter
    {\n### Remove unneeded fields came from fluent-bit\n  mutate {\n          remove_field
    => [ \"headers\",\"host\",\"date\"]\n      }\n\n  if [private_ip] {\n    ### Add
    temp field to get the correct host field inside NR\n    mutate {\n        add_field
    => { \"[@metadata][transf_host]\" => \"%{private_ip}\" }\n      }\n    mutate
    {\n        gsub => [ \"[@metadata][transf_host]\", \"\\.\", \"-\" ]\n        add_field
    => { \"host\" => \"ip-%{[@metadata][transf_host]}\" }\n      }\n  }      \n}\noutput
    {\n  newrelic {\n    license_key => \"${NR_LICENSE_KEY}\"\n    id => \"NR_out\"\n
    \ }\n}"
kind: ConfigMap
metadata:
  name: logstash-pipeline-nr-6tcc6f926g
  namespace: elastic-stack-logging
---
apiVersion: v1
data:
  s3.conf: |-
    input {
      http {
        port => 8081
        id => "s3_in"
      }
    }
    filter {
          ### Remove unneeded fields came from fluent-bit and set appropriate directory for s3
          mutate {
            add_field => { "[@metadata][s3_key]" => "%{[kubernetes][pod_name]}_%{[kubernetes][namespace_name]}_%{[kubernetes][container_name]}" }
            remove_field => [ "headers", "host","date"]
          }
    }
    output {
          s3 {
            region => "${AWS_REGION}"
            bucket => "${S3_BUCKET_NAME}"
            prefix => "/application/%{[@metadata][s3_key]}/%{+YYYY}/%{+MM}/%{+dd}/%{+HH}/%{+mm}/%{+ss}"
            time_file => "1"
            codec => "json"
            id => "s3_out"
          }
    }
kind: ConfigMap
metadata:
  name: logstash-pipeline-s3-gc9t2khd28
  namespace: elastic-stack-logging
---
apiVersion: v1
data:
  pipelines.yml: |-
    - pipeline.id: main
      path.config: "/usr/share/logstash/pipeline/main"
    - pipeline.id: customer
      path.config: "/usr/share/logstash/pipeline/customer"
    - pipeline.id: s3
      path.config: "/usr/share/logstash/pipeline/s3"
    - pipeline.id: cloudwatch
      path.config: "/usr/share/logstash/pipeline/cloudwatch"
    - pipeline.id: newrelic
      path.config: "/usr/share/logstash/pipeline/newrelic"
    - pipeline.id: dlq
      path.config: "/usr/share/logstash/pipeline/dlq"
      pipeline.workers: 1
kind: ConfigMap
metadata:
  labels:
    app: logstash-elastic
  name: logstash-pipelines
